{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b5e2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c46507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "train_data = np.loadtxt(r\"C:\\Users\\Ankit\\Downloads\\0000000000002417_training_boston_x_y_train.csv\",\n",
    "                        delimiter = \",\")\n",
    "X_train = train_data[:,:13]\n",
    "Y_Train = train_data[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c4e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import testing data\n",
    "X_test_data = np.loadtxt(r\"C:\\Users\\Ankit\\Downloads\\0000000000002417_test_boston_x_test.csv\",\n",
    "                        delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab956dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 182)\n",
      "(127, 182)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ankit\\AppData\\Local\\Temp\\ipykernel_12772\\291703083.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_training[st] = (data_training[i])*(data_training[j])\n",
      "C:\\Users\\Ankit\\AppData\\Local\\Temp\\ipykernel_12772\\291703083.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_testing[s_t] = (data_testing[i])*(data_testing[j])\n"
     ]
    }
   ],
   "source": [
    "# add some dummy data to training \n",
    "data_training = pd.DataFrame(X_train)\n",
    "column_len = len(data_training.values[0])\n",
    "st = 13\n",
    "for i in range(column_len):\n",
    "    for j in range(column_len):\n",
    "        data_training[st] = (data_training[i])*(data_training[j])\n",
    "        st +=1 \n",
    "        \n",
    "x_train = data_training.values\n",
    "print(x_train.shape)\n",
    "\n",
    "# add some dummy data to testing \n",
    "data_testing = pd.DataFrame(X_test_data)\n",
    "s_t = 13\n",
    "for i in range(column_len):\n",
    "    for j in range(column_len):\n",
    "        data_testing[s_t] = (data_testing[i])*(data_testing[j])\n",
    "        s_t +=1 \n",
    "        \n",
    "x_test = data_testing.values\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3bb76ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 182)\n",
      "182\n",
      "(127, 182)\n"
     ]
    }
   ],
   "source": [
    "# feature Scaling\n",
    "# for training data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "X_Train = scaler.transform(x_train) \n",
    "\n",
    "# for testing data\n",
    "X_Test = scaler.transform(x_test)\n",
    "print(X_Train.shape)\n",
    "print(len(X_Train[0]))\n",
    "print(X_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a82eb7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check score on training data\n",
    "def score(y_truth,y_pred):\n",
    "    u = ((y_truth-y_pred)**2).sum()\n",
    "    v = ((y_truth - y_truth.mean())**2).sum()\n",
    "    c_d = 1-(u/v)\n",
    "    return c_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37490b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict - predicting values for x_test\n",
    "def predict(X_data,m):\n",
    "    y_predict = []\n",
    "    for x_i in X_data:\n",
    "        c = np.ones(1)\n",
    "        x_c = np.concatenate((x_i,c))\n",
    "        \n",
    "        y = (m*x_c).sum()\n",
    "        y_predict.append(y)\n",
    "    y_predict = np.array(y_predict)\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8bdfa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost - finding the error on trainig_data\n",
    "def cost(X_Train,Y_Train,m):\n",
    "    total_cost = 0\n",
    "    M = len(X_Train)\n",
    "    for i in range(M):\n",
    "        x = X_Train[i]\n",
    "        y = Y_Train[i]\n",
    "        # c or intercept which is 1 in n-dimentional array\n",
    "        c = np.ones(1)\n",
    "        x_c = np.concatenate((x,c))\n",
    "        total_cost+= (1/M) * ((y-((m*x_c).sum()))** 2)\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c3fa0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_gradient - find the new regression coefficients\n",
    "def step_gredient(X_Train,Y_Train,learning_rate,m):\n",
    "    m_slope = np.zeros(len(X_Train[0])+1)\n",
    "    M = len(X_Train)\n",
    "    for i in range(M):\n",
    "        x = X_Train[i]\n",
    "        y = Y_Train[i]\n",
    "        # c or intercept which is 1 in n-dimentional array\n",
    "        c = np.ones(1)\n",
    "        x_c = np.concatenate((x,c))\n",
    "        for j in range(len(x_c)):\n",
    "            m_slope[j] += (-2 /M) * (y-(m*x_c).sum())*x_c[j]\n",
    "    new_m = m - learning_rate * m_slope\n",
    "    return new_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d9f0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic_gd - calculate regression coefficient with specified learning_rate and num_iterations\n",
    "def gd(X_Train,Y_Train,learning_rate,num_iteration):\n",
    "    m = np.zeros(len(X_Train[0])+1) # here +1 for c or intercept in n-dimentional\n",
    "    for i in range(num_iteration):\n",
    "        m = step_gredient(X_Train,Y_Train,learning_rate,m)\n",
    "        print(i,\"cost: \",cost(X_Train,Y_Train,m))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59589555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run - run gradient descent on training data\n",
    "def run(X_Train,Y_Train):\n",
    "    learning_rate  = 0.009\n",
    "    num_iteration = 350\n",
    "    m = gd(X_Train,Y_Train,learning_rate,num_iteration)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7c4508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost:  559.7100346638517\n",
      "1 cost:  531.1070953312225\n",
      "2 cost:  506.7760273539344\n",
      "3 cost:  485.10193115931355\n",
      "4 cost:  465.3265677444045\n",
      "5 cost:  447.00897850867835\n",
      "6 cost:  429.8679982475944\n",
      "7 cost:  413.7129191081911\n",
      "8 cost:  398.4081160378365\n",
      "9 cost:  383.85338502721845\n",
      "10 cost:  369.97226032476146\n",
      "11 cost:  356.704699337896\n",
      "12 cost:  344.0023164808187\n",
      "13 cost:  331.8251832156588\n",
      "14 cost:  320.1396292466127\n",
      "15 cost:  308.91670307683256\n",
      "16 cost:  298.1310768672502\n",
      "17 cost:  287.7602561518334\n",
      "18 cost:  277.7840018826844\n",
      "19 cost:  268.1839022820007\n",
      "20 cost:  258.94305161747775\n",
      "21 cost:  250.04580611341538\n",
      "22 cost:  241.4775960753687\n",
      "23 cost:  233.22477938541877\n",
      "24 cost:  225.27452574043494\n",
      "25 cost:  217.61472395760657\n",
      "26 cost:  210.23390675729314\n",
      "27 cost:  203.12118891920778\n",
      "28 cost:  196.26621577481004\n",
      "29 cost:  189.65911977032025\n",
      "30 cost:  183.2904833967518\n",
      "31 cost:  177.15130719546144\n",
      "32 cost:  171.23298185206176\n",
      "33 cost:  165.5272636177997\n",
      "34 cost:  160.02625246691707\n",
      "35 cost:  154.72237252622722\n",
      "36 cost:  149.60835441016582\n",
      "37 cost:  144.6772191687609\n",
      "38 cost:  139.9222636131884\n",
      "39 cost:  135.33704682798347\n",
      "40 cost:  130.91537771375053\n",
      "41 cost:  126.6513034316206\n",
      "42 cost:  122.53909864248243\n",
      "43 cost:  118.57325545144114\n",
      "44 cost:  114.74847398202242\n",
      "45 cost:  111.05965351603862\n",
      "46 cost:  107.50188414437248\n",
      "47 cost:  104.0704388816071\n",
      "48 cost:  100.7607662037883\n",
      "49 cost:  97.56848297390093\n",
      "50 cost:  94.48936772407778\n",
      "51 cost:  91.51935426729491\n",
      "52 cost:  88.65452561446895\n",
      "53 cost:  85.89110817556835\n",
      "54 cost:  83.22546622564246\n",
      "55 cost:  80.65409661865534\n",
      "56 cost:  78.173623733708\n",
      "57 cost:  75.78079463971038\n",
      "58 cost:  73.47247446584846\n",
      "59 cost:  71.2456419663055\n",
      "60 cost:  69.09738526868519\n",
      "61 cost:  67.02489779644254\n",
      "62 cost:  65.02547435639394\n",
      "63 cost:  63.09650738305161\n",
      "64 cost:  61.23548333213351\n",
      "65 cost:  59.4399792161323\n",
      "66 cost:  57.707659275314114\n",
      "67 cost:  56.036271777947405\n",
      "68 cost:  54.42364594395705\n",
      "69 cost:  52.86768898655078\n",
      "70 cost:  51.3663832666913\n",
      "71 cost:  49.917783555577785\n",
      "72 cost:  48.520014400572876\n",
      "73 cost:  47.17126759025776\n",
      "74 cost:  45.86979971452405\n",
      "75 cost:  44.61392981582495\n",
      "76 cost:  43.4020371278986\n",
      "77 cost:  42.23255889846081\n",
      "78 cost:  41.10398829253133\n",
      "79 cost:  40.01487237321381\n",
      "80 cost:  38.963810156898646\n",
      "81 cost:  37.94945073999384\n",
      "82 cost:  36.97049149441831\n",
      "83 cost:  36.0256763292146\n",
      "84 cost:  35.1137940157521\n",
      "85 cost:  34.23367657409884\n",
      "86 cost:  33.38419771824428\n",
      "87 cost:  32.56427135795083\n",
      "88 cost:  31.77285015510401\n",
      "89 cost:  31.008924132517844\n",
      "90 cost:  30.271519333236505\n",
      "91 cost:  29.55969652844956\n",
      "92 cost:  28.872549972214816\n",
      "93 cost:  28.209206201254183\n",
      "94 cost:  27.568822878155316\n",
      "95 cost:  26.950587676377623\n",
      "96 cost:  26.353717205523026\n",
      "97 cost:  25.77745597539145\n",
      "98 cost:  25.22107539739835\n",
      "99 cost:  24.68387282198468\n",
      "100 cost:  24.165170610703218\n",
      "101 cost:  23.664315241714913\n",
      "102 cost:  23.1806764474756\n",
      "103 cost:  22.7136463834409\n",
      "104 cost:  22.262638826659895\n",
      "105 cost:  21.82708840317137\n",
      "106 cost:  21.406449843156196\n",
      "107 cost:  21.00019726283863\n",
      "108 cost:  20.60782347216723\n",
      "109 cost:  20.228839307341005\n",
      "110 cost:  19.862772987281897\n",
      "111 cost:  19.509169493186928\n",
      "112 cost:  19.16758997032652\n",
      "113 cost:  18.837611151284765\n",
      "114 cost:  18.518824799867758\n",
      "115 cost:  18.210837174934582\n",
      "116 cost:  17.913268513432065\n",
      "117 cost:  17.625752531941544\n",
      "118 cost:  17.347935946070436\n",
      "119 cost:  17.07947800704641\n",
      "120 cost:  16.820050054894633\n",
      "121 cost:  16.56933508760181\n",
      "122 cost:  16.327027345691622\n",
      "123 cost:  16.09283191165825\n",
      "124 cost:  15.866464323723024\n",
      "125 cost:  15.64765020340054\n",
      "126 cost:  15.436124896377844\n",
      "127 cost:  15.231633126228685\n",
      "128 cost:  15.033928660502301\n",
      "129 cost:  14.842773988742634\n",
      "130 cost:  14.65794001200998\n",
      "131 cost:  14.47920574349255\n",
      "132 cost:  14.306358019810425\n",
      "133 cost:  14.139191222628344\n",
      "134 cost:  13.977507010208143\n",
      "135 cost:  13.821114058544557\n",
      "136 cost:  13.669827811741097\n",
      "137 cost:  13.523470241295085\n",
      "138 cost:  13.381869613972691\n",
      "139 cost:  13.244860267966569\n",
      "140 cost:  13.112282397039566\n",
      "141 cost:  12.983981842368587\n",
      "142 cost:  12.859809891813166\n",
      "143 cost:  12.739623086343094\n",
      "144 cost:  12.62328303336881\n",
      "145 cost:  12.510656226728104\n",
      "146 cost:  12.40161387309051\n",
      "147 cost:  12.29603172455031\n",
      "148 cost:  12.193789917186795\n",
      "149 cost:  12.094772815378438\n",
      "150 cost:  11.998868861665258\n",
      "151 cost:  11.905970431961233\n",
      "152 cost:  11.815973695925493\n",
      "153 cost:  11.728778482307904\n",
      "154 cost:  11.644288149091565\n",
      "155 cost:  11.562409458260587\n",
      "156 cost:  11.483052455028123\n",
      "157 cost:  11.406130351365343\n",
      "158 cost:  11.331559413677676\n",
      "159 cost:  11.259258854480423\n",
      "160 cost:  11.189150727930695\n",
      "161 cost:  11.121159829078348\n",
      "162 cost:  11.055213596702735\n",
      "163 cost:  10.991242019607826\n",
      "164 cost:  10.929177546251761\n",
      "165 cost:  10.868954997592217\n",
      "166 cost:  10.810511483032654\n",
      "167 cost:  10.753786319358934\n",
      "168 cost:  10.698720952559487\n",
      "169 cost:  10.645258882426369\n",
      "170 cost:  10.593345589837758\n",
      "171 cost:  10.54292846662661\n",
      "172 cost:  10.493956747942965\n",
      "173 cost:  10.446381447020931\n",
      "174 cost:  10.400155292265055\n",
      "175 cost:  10.355232666572858\n",
      "176 cost:  10.311569548814177\n",
      "177 cost:  10.269123457390215\n",
      "178 cost:  10.227853395798451\n",
      "179 cost:  10.187719800131529\n",
      "180 cost:  10.148684488441605\n",
      "181 cost:  10.110710611903412\n",
      "182 cost:  10.073762607712089\n",
      "183 cost:  10.037806153653944\n",
      "184 cost:  10.002808124290448\n",
      "185 cost:  9.968736548698251\n",
      "186 cost:  9.935560569709487\n",
      "187 cost:  9.90325040459919\n",
      "188 cost:  9.8717773071681\n",
      "189 cost:  9.841113531171343\n",
      "190 cost:  9.811232295044935\n",
      "191 cost:  9.78210774788397\n",
      "192 cost:  9.753714936627826\n",
      "193 cost:  9.726029774409648\n",
      "194 cost:  9.69902901002842\n",
      "195 cost:  9.672690198503865\n",
      "196 cost:  9.646991672675426\n",
      "197 cost:  9.621912515808436\n",
      "198 cost:  9.597432535171489\n",
      "199 cost:  9.573532236550347\n",
      "200 cost:  9.550192799665375\n",
      "201 cost:  9.527396054460064\n",
      "202 cost:  9.505124458229814\n",
      "203 cost:  9.48336107356102\n",
      "204 cost:  9.46208954705181\n",
      "205 cost:  9.441294088786316\n",
      "206 cost:  9.420959452536055\n",
      "207 cost:  9.401070916662224\n",
      "208 cost:  9.38161426569435\n",
      "209 cost:  9.362575772560914\n",
      "210 cost:  9.343942181448998\n",
      "211 cost:  9.325700691270486\n",
      "212 cost:  9.307838939713333\n",
      "213 cost:  9.290344987857095\n",
      "214 cost:  9.2732073053326\n",
      "215 cost:  9.256414756006544\n",
      "216 cost:  9.239956584172278\n",
      "217 cost:  9.22382240122893\n",
      "218 cost:  9.208002172831213\n",
      "219 cost:  9.192486206493612\n",
      "220 cost:  9.177265139632595\n",
      "221 cost:  9.162329928031228\n",
      "222 cost:  9.147671834711314\n",
      "223 cost:  9.133282419198654\n",
      "224 cost:  9.119153527167281\n",
      "225 cost:  9.105277280449346\n",
      "226 cost:  9.091646067397708\n",
      "227 cost:  9.078252533588575\n",
      "228 cost:  9.065089572852282\n",
      "229 cost:  9.052150318620528\n",
      "230 cost:  9.039428135578664\n",
      "231 cost:  9.026916611612554\n",
      "232 cost:  9.014609550039244\n",
      "233 cost:  9.002500962111599\n",
      "234 cost:  8.990585059786977\n",
      "235 cost:  8.97885624875083\n",
      "236 cost:  8.967309121685986\n",
      "237 cost:  8.95593845177908\n",
      "238 cost:  8.94473918645543\n",
      "239 cost:  8.933706441334778\n",
      "240 cost:  8.922835494399443\n",
      "241 cost:  8.91212178036792\n",
      "242 cost:  8.901560885266287\n",
      "243 cost:  8.891148541190544\n",
      "244 cost:  8.880880621253151\n",
      "245 cost:  8.870753134707188\n",
      "246 cost:  8.860762222241869\n",
      "247 cost:  8.850904151443302\n",
      "248 cost:  8.841175312414743\n",
      "249 cost:  8.831572213550634\n",
      "250 cost:  8.822091477458947\n",
      "251 cost:  8.812729837026682\n",
      "252 cost:  8.803484131623408\n",
      "253 cost:  8.794351303437896\n",
      "254 cost:  8.785328393943242\n",
      "255 cost:  8.77641254048602\n",
      "256 cost:  8.767600972994703\n",
      "257 cost:  8.758891010803616\n",
      "258 cost:  8.750280059588077\n",
      "259 cost:  8.74176560840667\n",
      "260 cost:  8.733345226847295\n",
      "261 cost:  8.725016562272867\n",
      "262 cost:  8.716777337163377\n",
      "263 cost:  8.708625346550956\n",
      "264 cost:  8.700558455544547\n",
      "265 cost:  8.692574596940965\n",
      "266 cost:  8.684671768919554\n",
      "267 cost:  8.67684803281717\n",
      "268 cost:  8.669101510980896\n",
      "269 cost:  8.661430384695697\n",
      "270 cost:  8.653832892184186\n",
      "271 cost:  8.646307326676315\n",
      "272 cost:  8.63885203454612\n",
      "273 cost:  8.631465413513535\n",
      "274 cost:  8.62414591090869\n",
      "275 cost:  8.616892021996641\n",
      "276 cost:  8.609702288360369\n",
      "277 cost:  8.602575296340015\n",
      "278 cost:  8.595509675526369\n",
      "279 cost:  8.588504097306563\n",
      "280 cost:  8.581557273460447\n",
      "281 cost:  8.574667954805534\n",
      "282 cost:  8.567834929889\n",
      "283 cost:  8.561057023724974\n",
      "284 cost:  8.554333096575686\n",
      "285 cost:  8.54766204277476\n",
      "286 cost:  8.541042789591177\n",
      "287 cost:  8.534474296132643\n",
      "288 cost:  8.527955552286874\n",
      "289 cost:  8.521485577699378\n",
      "290 cost:  8.515063420786602\n",
      "291 cost:  8.508688157783148\n",
      "292 cost:  8.502358891821858\n",
      "293 cost:  8.496074752045576\n",
      "294 cost:  8.48983489274953\n",
      "295 cost:  8.483638492553283\n",
      "296 cost:  8.477484753601177\n",
      "297 cost:  8.471372900790174\n",
      "298 cost:  8.465302181024411\n",
      "299 cost:  8.459271862495163\n",
      "300 cost:  8.453281233985667\n",
      "301 cost:  8.447329604199755\n",
      "302 cost:  8.441416301113431\n",
      "303 cost:  8.435540671348747\n",
      "304 cost:  8.429702079569026\n",
      "305 cost:  8.423899907894821\n",
      "306 cost:  8.418133555339898\n",
      "307 cost:  8.41240243726624\n",
      "308 cost:  8.40670598485796\n",
      "309 cost:  8.401043644612914\n",
      "310 cost:  8.395414877851737\n",
      "311 cost:  8.389819160243542\n",
      "312 cost:  8.384255981347868\n",
      "313 cost:  8.378724844171948\n",
      "314 cost:  8.373225264743292\n",
      "315 cost:  8.36775677169656\n",
      "316 cost:  8.362318905874503\n",
      "317 cost:  8.356911219942495\n",
      "318 cost:  8.351533278015946\n",
      "319 cost:  8.346184655300407\n",
      "320 cost:  8.340864937743827\n",
      "321 cost:  8.33557372170053\n",
      "322 cost:  8.330310613606438\n",
      "323 cost:  8.325075229665318\n",
      "324 cost:  8.319867195545562\n",
      "325 cost:  8.314686146087116\n",
      "326 cost:  8.30953172501822\n",
      "327 cost:  8.30440358468178\n",
      "328 cost:  8.29930138577068\n",
      "329 cost:  8.294224797072138\n",
      "330 cost:  8.289173495220478\n",
      "331 cost:  8.284147164458236\n",
      "332 cost:  8.279145496405127\n",
      "333 cost:  8.274168189834798\n",
      "334 cost:  8.269214950458938\n",
      "335 cost:  8.264285490718558\n",
      "336 cost:  8.259379529582223\n",
      "337 cost:  8.254496792350904\n",
      "338 cost:  8.249637010469371\n",
      "339 cost:  8.244799921343713\n",
      "340 cost:  8.239985268164938\n",
      "341 cost:  8.23519279973834\n",
      "342 cost:  8.230422270318515\n",
      "343 cost:  8.225673439449778\n",
      "344 cost:  8.22094607181176\n",
      "345 cost:  8.21623993707016\n",
      "346 cost:  8.21155480973226\n",
      "347 cost:  8.206890469007236\n",
      "348 cost:  8.202246698670967\n",
      "349 cost:  8.197623286935269\n"
     ]
    }
   ],
   "source": [
    "m = run(X_Train,Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df280e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379,)\n",
      "training_score:  0.9067740400115002\n"
     ]
    }
   ],
   "source": [
    "# findout y predict for training data\n",
    "y_pred_traing = predict(X_Train,m)\n",
    "print(y_pred_traing.shape)\n",
    "\n",
    "# checking the score on testing data\n",
    "training_score = score(Y_Train,y_pred_traing)\n",
    "print(\"training_score: \",training_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# findout y predict for testing data\n",
    "y_pred_testing = predict(X_Test,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109160ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np array convert in to csv file:-\n",
    "np.savetxt(\"Gradient_Descent_Boston_Dataset_predict.csv\", y_pred_testing,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972591d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
